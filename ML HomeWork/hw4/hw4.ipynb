{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA5wWLs4YrNR"
   },
   "source": [
    "# Applied Machine Learning \n",
    "\n",
    "## Homework 4: Logistic regression, hyperparameter optimization \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wSdY3ZSYrNd"
   },
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dmGwJI_UYrNe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wma2gLiiYrNg"
   },
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I55tp2qsYrNh"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "432U52LDYrNi"
   },
   "source": [
    "## Exercise 1: Implementing `DummyClassifier` \n",
    "<hr>\n",
    "rubric={points:25}\n",
    "\n",
    "In this course (unlike CPSC 340) you will generally **not** be asked to implement machine learning algorihtms (like logistic regression) from scratch. However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    "\n",
    "As a reminder, `DummyClassifier` is meant as a baseline and is generally the worst possible \"model\" you could \"fit\" to a dataset. All it does is predict the most popular class in the training set. So if there are more 0s than 1s it predicts 0 every time, and if there are more 1s than 0s it predicts 1 every time. For `predict_proba` it looks at the frequencies in the training set, so if you have 30% 0's 70% 1's it predicts `[0.3 0.7]` every time. Thus, `fit` only looks at `y` (not `X`).\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, `predict_proba()` and `score()`. Your task is to fill in those four functions. To get your started, I have given you a `return` statement in each case that returns the correct data type: `fit` can return nothing, `predict` returns an array whose size is the number of examples, `predict_proba` returns an array whose size is the number of examples x 2, and `score` returns a number.\n",
    "\n",
    "The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the frequency of each class in the training data, which is the behaviour of `DummyClassifier(strategy='prior')`. Your `score` function should call your `predict` function. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "To simplify this question, you can assume **binary classification**, and furthermore that these classes are **encoded as 0 and 1**. In other words, you can assume that `y` contains only 0s and 1s. The real `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"mouse\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BNDaKzkfYrNk"
   },
   "outputs": [],
   "source": [
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "\n",
    "    This implementation only works when y only contains 0s and 1s.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dummyc = DummyClassifier(strategy=\"prior\")\n",
    "        dummy_fit = dummyc.fit(X, y)\n",
    "        return dummy_fit  # Replace with your code\n",
    "\n",
    "    def predict(self, X):\n",
    "        dummy_fit = self.fit(X_train_dummy, y_train_dummy)\n",
    "        return dummy_fit.predict(X)  # Replace with your code\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        dummy_fit = self.fit(X_train_dummy, y_train_dummy)\n",
    "        return dummy_fit.predict_proba(X)  # Replace with your code\n",
    "\n",
    "    def score(self, X, y):\n",
    "        dummy_fit = self.fit(X_train_dummy, y_train_dummy)\n",
    "        return dummy_fit.score(X, y)  # Replace with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujQ6MBTZYrNk"
   },
   "source": [
    "Below are some tests for `predict` using randomly generated data. You may want to run the cell a few times to make sure you explore the different cases (or automate this with a loop or random seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e90LLle8YrNl"
   },
   "outputs": [],
   "source": [
    "# For testing, generate random data\n",
    "n_train = 101\n",
    "n_valid = 21\n",
    "d = 5\n",
    "X_train_dummy = np.random.randn(n_train, d)\n",
    "X_valid_dummy = np.random.randn(n_valid, d)\n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "\n",
    "my_dc = MyDummyClassifier()\n",
    "sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "\n",
    "assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TporcFoWYrNm"
   },
   "source": [
    "Below are some tests for `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ia39rfVpYrNm"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy)\n",
    ")\n",
    "assert np.allclose(\n",
    "    my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaM_KREHYrNn"
   },
   "source": [
    "Below are some tests for `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oEP6dYvnYrNo"
   },
   "outputs": [],
   "source": [
    "assert np.isclose(\n",
    "    my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy)\n",
    ")\n",
    "assert np.isclose(\n",
    "    my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0GE7tlpYrNo"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zEJ7fjWYrNo",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e3cc53df86a7e14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 2: Trump Tweets\n",
    "<hr>\n",
    "\n",
    "For the rest of this assignment we'll be looking at a [dataset of Donald Trump's tweets](https://www.kaggle.com/austinreese/trump-tweets) as of June 2020. You should start by downloading the dataset. Unzip it and move the file `realdonaldtrump.csv` into this directory. As usual, please do not submit the dataset when you submit the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G97JVtpeYrNp"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         link  \\\n",
       "id                                                              \n",
       "1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                                      content  \\\n",
       "id                                                              \n",
       "1698308935  Be sure to tune in and watch Donald Trump on L...   \n",
       "1701461182  Donald Trump will be appearing on The View tom...   \n",
       "1737479987  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "1741160716  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "1773561338  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                           date  retweets  favorites mentions hashtags  \n",
       "id                                                                      \n",
       "1698308935  2009-05-04 13:54:25       510        917      NaN      NaN  \n",
       "1701461182  2009-05-04 20:00:10        34        267      NaN      NaN  \n",
       "1737479987  2009-05-08 08:38:08        13         19      NaN      NaN  \n",
       "1741160716  2009-05-08 15:40:15        11         26      NaN      NaN  \n",
       "1773561338  2009-05-12 09:07:28      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"realdonaldtrump.csv\", index_col=0)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4bvCYDrCYrNp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43352, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kkkJDzQYrNq"
   },
   "source": [
    "We will be trying to predict whether a tweet will go \"viral\", defined as having more than 10,000 retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0oo3gLVmYrNr"
   },
   "outputs": [],
   "source": [
    "y = tweets_df[\"retweets\"] > 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sO8bOnhYrNs"
   },
   "source": [
    "To make predictions, we'll be using only the content (text) of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "py-EbduaYrNs"
   },
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO0lsy84YrNs"
   },
   "source": [
    "For the purpose of this assignment, you can ignore all the other columns in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zCbm6vYYrNt"
   },
   "source": [
    "#### 2(a) ordering the steps\n",
    "rubric={points:8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Nle26hNYrNt"
   },
   "source": [
    "Let's start by building a model using `CountVectorizer` and `LogisticRegression`. The code required to do this has been provided below, but in the wrong order. \n",
    "\n",
    "- Rearrange the lines of code to correctly fit the model and compute the cross-validation score. \n",
    "- Add a short comment to each block to describe what the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-BEq07JXYrNt",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       1.561848\n",
       "score_time     0.128800\n",
       "test_score     0.897890\n",
       "train_score    0.967045\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spiliting Train and Test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=321)\n",
    "\n",
    "# Implementing LogisticRegression \n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Implememting CountVectorizer\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# making pipeline\n",
    "pipe = make_pipeline(countvec, lr)\n",
    "\n",
    "# cross validating the data\n",
    "cross_val_results = pd.DataFrame(\n",
    "    cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    ")\n",
    "\n",
    "# cross validate mean score\n",
    "cross_val_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoWJaaJTYrNu"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UcepQcXYrNu"
   },
   "source": [
    "#### 2(b) Cross-validation fold sub-scores\n",
    "rubric={points:5}\n",
    "\n",
    "Above we averaged the scores from the 5 folds of cross-validation. \n",
    "\n",
    "- Print out the 5 individual scores. Reminder: `sklearn` calls them `\"test_score\"` but they are really (cross-)validation scores. \n",
    "- Are the 5 scores close to each other or spread far apart? (This is a bit subjective, answer to the best of your ability.)\n",
    "- How does the size of this dataset (number of rows) compare to the cities dataset we have been using in class? How does this relate to the different sub-scores from the 5 folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_val_score(pipe, X_train, y_train, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89912348, 0.89973858, 0.89635553, 0.89820083, 0.89603199])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeLuNaNiYrNu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoa1TJjGYrNv"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf5SOBuSYrNv"
   },
   "source": [
    "#### 2(c) baseline\n",
    "rubric={points:3}\n",
    "\n",
    "By the way, are these scores any good? \n",
    "\n",
    "- Run `DummyClassifier` (or `MyDummyClassifier`!) on this dataset.\n",
    "- Compare the `DummyClassifier` score to what you got from logistic regression above. Does logistic regression seem to be doing anything useful?\n",
    "- Is it necessary to use `CountVectorizer` here? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1rPtV1jYrNv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7313157409116073"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy_score = dummy.score(X_test, y_test)\n",
    "dummy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9652457402964877"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = LogisticRegression(max_iter=1000)\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "pipe = make_pipeline(countvec, linear)\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = LogisticRegression(max_iter=1000)\n",
    "# pipe = make_pipeline(linear)\n",
    "# pipe.fit(X_train, y_train)\n",
    "# linear_score = pipe.score(X_train, y_train)\n",
    "# linear_score\n",
    "\n",
    "# CounterVectorizer is important as LogisticRegression could not convert string to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH3IpZT9YrNv",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f8ea22638cf75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2(d) probability scores\n",
    "rubric={points:5}\n",
    "\n",
    "Here we train a logistic regression classifier on the entire training set: \n",
    "\n",
    "(Note: this is relying on the `pipe` variable from 2(a) - you'll need to redefine it if you overwrote that variable in between.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZP1Xx2H5YrNw"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHZ4or-NYrNw"
   },
   "source": [
    "Using this model, find the tweet in the **test set** with the highest predicted probability of being viral. Print out the tweet and the associated probability score.\n",
    "\n",
    "Reminder: you are free to reuse/adapt code from lecture. Please add in a small attribution, e.g. \"From Lecture 7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>vir_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5134</th>\n",
       "      <td>1193603788898263041</td>\n",
       "      <td>Corrupt politician Adam Schiff wants people fr...</td>\n",
       "      <td>9.999999e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>1178446179320942593</td>\n",
       "      <td>These Radical Left, Do Nothing Democrats, are ...</td>\n",
       "      <td>9.999997e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8625</th>\n",
       "      <td>1169981019228913664</td>\n",
       "      <td>....This nonsense has never happened to anothe...</td>\n",
       "      <td>9.999994e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>1269761532033937412</td>\n",
       "      <td>If I wasn’t constantly harassed for three year...</td>\n",
       "      <td>9.999994e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>1114888062884954114</td>\n",
       "      <td>Looks like Bob Mueller’s team of 13 Trump Hate...</td>\n",
       "      <td>9.999988e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10433</th>\n",
       "      <td>523206147298562048</td>\n",
       "      <td>Via @ DailyCaller by @ AlexPappas: “Donald Tru...</td>\n",
       "      <td>3.675607e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>544594336412086272</td>\n",
       "      <td>Via @ TVGrapevine: “ @ ApprenticeNBC: Premiere...</td>\n",
       "      <td>3.128320e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>559418862975074305</td>\n",
       "      <td>Via @ NYDailyNews by Rich Schapiro: \"Donald Tr...</td>\n",
       "      <td>1.905989e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>509764209446752256</td>\n",
       "      <td>“ @ ApprenticeNBC: Donald Trump Talks Joan Riv...</td>\n",
       "      <td>8.720862e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>656553742859374592</td>\n",
       "      <td>Lots of great new polls--big leads! http://www...</td>\n",
       "      <td>1.022202e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10838 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                            content  \\\n",
       "5134   1193603788898263041  Corrupt politician Adam Schiff wants people fr...   \n",
       "3070   1178446179320942593  These Radical Left, Do Nothing Democrats, are ...   \n",
       "8625   1169981019228913664  ....This nonsense has never happened to anothe...   \n",
       "8695   1269761532033937412  If I wasn’t constantly harassed for three year...   \n",
       "1534   1114888062884954114  Looks like Bob Mueller’s team of 13 Trump Hate...   \n",
       "...                    ...                                                ...   \n",
       "10433   523206147298562048  Via @ DailyCaller by @ AlexPappas: “Donald Tru...   \n",
       "9973    544594336412086272  Via @ TVGrapevine: “ @ ApprenticeNBC: Premiere...   \n",
       "6365    559418862975074305  Via @ NYDailyNews by Rich Schapiro: \"Donald Tr...   \n",
       "3307    509764209446752256  “ @ ApprenticeNBC: Donald Trump Talks Joan Riv...   \n",
       "5620    656553742859374592  Lots of great new polls--big leads! http://www...   \n",
       "\n",
       "          vir_score  \n",
       "5134   9.999999e-01  \n",
       "3070   9.999997e-01  \n",
       "8625   9.999994e-01  \n",
       "8695   9.999994e-01  \n",
       "1534   9.999988e-01  \n",
       "...             ...  \n",
       "10433  3.675607e-07  \n",
       "9973   3.128320e-07  \n",
       "6365   1.905989e-07  \n",
       "3307   8.720862e-08  \n",
       "5620   1.022202e-09  \n",
       "\n",
       "[10838 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vir_score = pipe.predict_proba(X_test)[:,1]\n",
    "X_test_dup = X_test.reset_index()\n",
    "X_test_dup['vir_score'] = vir_score \n",
    "X_test_dup.sort_values(by = 'vir_score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROj-OXpxYrNw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbhJH2qtYrNw"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhWWLEgnYrNx",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f910e9d1d6d09182",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2(e) coefficients\n",
    "rubric={points:4}\n",
    "\n",
    "We can extract the `CountVectorizer` and `LogisticRegression` objects from the `make_pipeline` object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jYeFDvadYrNx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer(stop_words='english')),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_from_pipe = pipe.named_steps[\"countvectorizer\"]\n",
    "lr_from_pipe = pipe.named_steps[\"logisticregression\"]\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0002',\n",
       " '00021',\n",
       " '00022',\n",
       " '000hermansjr000',\n",
       " '000th',\n",
       " '000this',\n",
       " '005',\n",
       " '0062343971',\n",
       " '007cigarjoe',\n",
       " '007llisav',\n",
       " '0098a264184d',\n",
       " '00am',\n",
       " '00gnh9nlet',\n",
       " '00hol0to0u',\n",
       " '00mao6vk7r',\n",
       " '00p',\n",
       " '00pm',\n",
       " '00pme',\n",
       " '00pmest',\n",
       " '00rstw00',\n",
       " '00ykww_bq2i',\n",
       " '01',\n",
       " '01101o10',\n",
       " '01am',\n",
       " '02',\n",
       " '020215',\n",
       " '0207',\n",
       " '022pjwhhjs',\n",
       " '03',\n",
       " '032',\n",
       " '0324760ca856_story',\n",
       " '036',\n",
       " '03d039e997c2',\n",
       " '03ddyvu6ha',\n",
       " '03uzbgcswb',\n",
       " '04',\n",
       " '040',\n",
       " '040715_trumpferrypoint',\n",
       " '041ccb32',\n",
       " '044xgnylth',\n",
       " '04qqpgylt7',\n",
       " '05',\n",
       " '050urjw3ay',\n",
       " '05ce',\n",
       " '05fxdli',\n",
       " '05xrx2odxn',\n",
       " '06',\n",
       " '0624152016iranweb',\n",
       " '062clmcqan',\n",
       " '065ba39beb30',\n",
       " '067',\n",
       " '06wj5cdqjee',\n",
       " '07',\n",
       " '071',\n",
       " '07_aoamhqe',\n",
       " '08',\n",
       " '081',\n",
       " '0822',\n",
       " '08fmnhytac',\n",
       " '08xkebd8be',\n",
       " '09',\n",
       " '0900',\n",
       " '0930',\n",
       " '09a7998e75f145e',\n",
       " '09bac03rx6',\n",
       " '0a25gapyj6',\n",
       " '0a7apd4kuo',\n",
       " '0angxb2bri',\n",
       " '0auxm3mkpr',\n",
       " '0bama',\n",
       " '0bstlvq7v9',\n",
       " '0c004f23943c33b9',\n",
       " '0cdd44c0b576',\n",
       " '0cidcuofeo',\n",
       " '0dhgkdghrn',\n",
       " '0dtxsagkz1',\n",
       " '0ei3edqdxb',\n",
       " '0evxj7ww6c',\n",
       " '0f9f9c80f13f',\n",
       " '0fbis3',\n",
       " '0ftv64flypcg0p4pnxauuoxlacsaqv',\n",
       " '0g9fz',\n",
       " '0gj8tira5y',\n",
       " '0gwmhgz56k58_gydaco3tnkq',\n",
       " '0jcjfbcpjz',\n",
       " '0ksfxiokdg',\n",
       " '0lcn8awguw',\n",
       " '0luzczl29d',\n",
       " '0lw2',\n",
       " '0mdeymze3ndf8mxlvs01eqmfnymx4uxtau1dfdbs2hr83rlldpoot9n_c3w',\n",
       " '0n4aeorwk9',\n",
       " '0ngiihgy18',\n",
       " '0nlypablo',\n",
       " '0nokuuvuuv',\n",
       " '0nonsense',\n",
       " '0nqlzbwufo',\n",
       " '0ouupojeq4',\n",
       " '0psrxmxshu',\n",
       " '0pvuuvf2vy',\n",
       " '0qusxersql',\n",
       " '0r81wgq4ba',\n",
       " '0rhkz5ct',\n",
       " '0uct0qvyo6',\n",
       " '0unicornchris0',\n",
       " '0vjzleehtl',\n",
       " '0xbloood',\n",
       " '0yxynumyxj',\n",
       " '0zx9wr3mop',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '1000060619108712449',\n",
       " '10000th',\n",
       " '1000m',\n",
       " '1001',\n",
       " '100114',\n",
       " '10022',\n",
       " '1002760314734170113',\n",
       " '100381628',\n",
       " '100614',\n",
       " '1007745343306584064',\n",
       " '1009438008733569025',\n",
       " '100b',\n",
       " '100beachbumapp',\n",
       " '100g',\n",
       " '100k',\n",
       " '100m',\n",
       " '100th',\n",
       " '100x',\n",
       " '100yrs',\n",
       " '101',\n",
       " '1010500676269404166',\n",
       " '1010winsnewyork',\n",
       " '101102',\n",
       " '1011253664961433600',\n",
       " '1012041519866531841',\n",
       " '10129948',\n",
       " '101414',\n",
       " '1015244162315309057',\n",
       " '10154473297477835',\n",
       " '10154879521400725',\n",
       " '10156034501145725',\n",
       " '10156376798595725',\n",
       " '10156377233170725',\n",
       " '10156387656245725',\n",
       " '10156395027595725',\n",
       " '10156396313845725',\n",
       " '10156398335585725',\n",
       " '10156401840540725',\n",
       " '10156402109300725',\n",
       " '10156403206355725',\n",
       " '10156403687515725',\n",
       " '10156405353795725',\n",
       " '10156406326945725',\n",
       " '10156407894785725',\n",
       " '10156412088210725',\n",
       " '10156412511925725',\n",
       " '10156412899695725',\n",
       " '10156414357265725',\n",
       " '10156414635755725',\n",
       " '10156417157260725',\n",
       " '10156417390615725',\n",
       " '10156425109365725',\n",
       " '10156427445680725',\n",
       " '10156436262810725',\n",
       " '10156440107765725',\n",
       " '10156442220085725',\n",
       " '10156443020965725',\n",
       " '10156444812435725',\n",
       " '10156445430005725',\n",
       " '10156453684740725',\n",
       " '10156460622260725',\n",
       " '10156462054585725',\n",
       " '10156467989725725',\n",
       " '10156473232605725',\n",
       " '10156475262795725',\n",
       " '10156475870665725',\n",
       " '10156478841470725',\n",
       " '10156479246710725',\n",
       " '10156480193140725',\n",
       " '10156500910690725',\n",
       " '10156502334810725',\n",
       " '10156529697320725',\n",
       " '10156540847600725',\n",
       " '10156544702880725',\n",
       " '10156546300015725',\n",
       " '10156548303850725',\n",
       " '10156549336385725',\n",
       " '10156549868870725',\n",
       " '10156552826965725',\n",
       " '10156553066190725',\n",
       " '10156556845290725',\n",
       " '10156559989840725',\n",
       " '10156563262030725',\n",
       " '10156563265515725',\n",
       " '10156570856100725',\n",
       " '10156574346040725',\n",
       " '10156578006175725',\n",
       " '10156581958480725',\n",
       " '10156583139705725',\n",
       " '10156583412010725',\n",
       " '10156584028145725',\n",
       " '10156584214900725',\n",
       " '10156585686910725',\n",
       " '10156591076765725',\n",
       " '10156599412210725',\n",
       " '10156602612915725',\n",
       " '10156604838290725',\n",
       " '10156608890980725',\n",
       " '10156610923070725',\n",
       " '10156616498145725',\n",
       " '10156620333635725',\n",
       " '10156623696310725',\n",
       " '10156626180645725',\n",
       " '10156638082135725',\n",
       " '10156642768615725',\n",
       " '10156650738555725',\n",
       " '10156655304665725',\n",
       " '10156655608040725',\n",
       " '10156658168535725',\n",
       " '10156659139295725',\n",
       " '10156671493865725',\n",
       " '10156674068880725',\n",
       " '10156676695120725',\n",
       " '10156679258630725',\n",
       " '10156680424255725',\n",
       " '10156681405075725',\n",
       " '10156683134825725',\n",
       " '10156687171790725',\n",
       " '10156689716915725',\n",
       " '10156691124655725',\n",
       " '10156696078600725',\n",
       " '10156699096370725',\n",
       " '10156699675010725',\n",
       " '10156700735560725',\n",
       " '10156703438065725',\n",
       " '10156706347280725',\n",
       " '10156710114610725',\n",
       " '10156710122455725',\n",
       " '10156711173905725',\n",
       " '10156714551570725',\n",
       " '10156715088630725',\n",
       " '10156717396415725',\n",
       " '10156717964125725',\n",
       " '10156724047250725',\n",
       " '10156730921700725',\n",
       " '10156744558065725',\n",
       " '10156748929285725',\n",
       " '10156808566585725',\n",
       " '10156861405690725',\n",
       " '10156862675620725',\n",
       " '10156866194350725',\n",
       " '10156881962720725',\n",
       " '10156883852800725',\n",
       " '10156887510055725',\n",
       " '10156940562995725',\n",
       " '10156952559355725',\n",
       " '10156977481470725',\n",
       " '10156999573740725',\n",
       " '10157017995990725',\n",
       " '10157139281305725',\n",
       " '10157139363145725',\n",
       " '10157139384175725',\n",
       " '10157139440985725',\n",
       " '10157164046265725',\n",
       " '10157164117925725',\n",
       " '10157164318560725',\n",
       " '10157164335365725',\n",
       " '10157169523180725',\n",
       " '10157185637660725',\n",
       " '10157210293405725',\n",
       " '10157213077740725',\n",
       " '10157219372685725',\n",
       " '10157225458500725',\n",
       " '10157246984980725',\n",
       " '10157254332085725',\n",
       " '10157263351880725',\n",
       " '10157277205810725',\n",
       " '10157280014500725',\n",
       " '10157283903165725',\n",
       " '10157294436575725',\n",
       " '10157298677315725',\n",
       " '10157329790520725',\n",
       " '10157337060605725',\n",
       " '10157368945620725',\n",
       " '10157369699375725',\n",
       " '10157370239030725',\n",
       " '10157370422355725',\n",
       " '10157382674030725',\n",
       " '10157388173350725',\n",
       " '10157391232965725',\n",
       " '10157391345450725',\n",
       " '10157422221965725',\n",
       " '10157449657485725',\n",
       " '10157458402275725',\n",
       " '10157472447805725',\n",
       " '10157509540605725',\n",
       " '10157522097930725',\n",
       " '10157542172590725',\n",
       " '10157608417635725',\n",
       " '10157627283230725',\n",
       " '10157690605840725',\n",
       " '10157759149590725',\n",
       " '10157785229045725',\n",
       " '10157791494485725',\n",
       " '10157856656835725',\n",
       " '10157884961280725',\n",
       " '10157932697660725',\n",
       " '10157934436595725',\n",
       " '10157937570415725',\n",
       " '10157946929405725',\n",
       " '10157952895095725',\n",
       " '10157958946770725',\n",
       " '10157973074675725',\n",
       " '10157980288935725',\n",
       " '10157984269835725',\n",
       " '10157990081530725',\n",
       " '10157990104410725',\n",
       " '10157991657715725',\n",
       " '10157991985830725',\n",
       " '10158015875825725',\n",
       " '10158017552765725',\n",
       " '10158018669815725',\n",
       " '10158023562820725',\n",
       " '10158050611675725',\n",
       " '10158057072840725',\n",
       " '10158062116155725',\n",
       " '10158066779845725',\n",
       " '10158071461455725',\n",
       " '10158072146545725',\n",
       " '10158074771180725',\n",
       " '10158078712110725',\n",
       " '10158079260770725',\n",
       " '10158080188865725',\n",
       " '10158081600190725',\n",
       " '10158207891610725',\n",
       " '10158231301535725',\n",
       " '10158244280375725',\n",
       " '10158266355460725',\n",
       " '10158305240145725',\n",
       " '10158310632295725',\n",
       " '10158338539250725',\n",
       " '10158426170160725',\n",
       " '10158443103370725',\n",
       " '10158510987625725',\n",
       " '10158635419120725',\n",
       " '10158646970740725',\n",
       " '10158684611165725',\n",
       " '10159027593260725',\n",
       " '10159677763575725',\n",
       " '10159704959520725',\n",
       " '10159709733805725',\n",
       " '10159801166360725',\n",
       " '10160226151710725',\n",
       " '10162491434245725',\n",
       " '1016306232104275968',\n",
       " '1017288317342224384',\n",
       " '1018202558764945408',\n",
       " '1018am',\n",
       " '102',\n",
       " '1020144866012073991',\n",
       " '10206020703413100',\n",
       " '1021536193710907423',\n",
       " '1022291789678628864',\n",
       " '1022699584987049990',\n",
       " '1024',\n",
       " '1024th',\n",
       " '1025064933736759298',\n",
       " '1025471369599901696',\n",
       " '1025743163661594624',\n",
       " '1028816390663426048',\n",
       " '1029',\n",
       " '102nd',\n",
       " '103',\n",
       " '103015',\n",
       " '1033058964651618304',\n",
       " '1033116113704505344',\n",
       " '1034128472946434056',\n",
       " '1036690580531757063',\n",
       " '1037002996436148230',\n",
       " '1037107325771948032',\n",
       " '1037740482988834821',\n",
       " '1037am',\n",
       " '1039921383944413184',\n",
       " '1040235446876467201',\n",
       " '1040631845145194498',\n",
       " '1040702223901241344',\n",
       " '1041632865853890560',\n",
       " '1044604674253508609',\n",
       " '1044szl',\n",
       " '1045820275831402496',\n",
       " '1047112237612183552',\n",
       " '1047286083766112256',\n",
       " '1047615925078425600',\n",
       " '1049376840975101953',\n",
       " '105',\n",
       " '1050132146373967892',\n",
       " '1050164476916523008',\n",
       " '1050900270979403776',\n",
       " '1051523821642113025',\n",
       " '1052174007708147714',\n",
       " '1052589131732267009',\n",
       " '1052984302357508096',\n",
       " '1053448472156749824',\n",
       " '1054296304619343872',\n",
       " '105445',\n",
       " '1055115436361166849',\n",
       " '1055636472144035840',\n",
       " '1055875113839747072',\n",
       " '1055984678648406016',\n",
       " '1055986341211791360',\n",
       " '1056315102327201793',\n",
       " '1058049612924313601',\n",
       " '1058155499202510849',\n",
       " '1059138871039008769',\n",
       " '1059224573957283842',\n",
       " '1063561072731279360',\n",
       " '1064200005584588800',\n",
       " '106yocl',\n",
       " '107',\n",
       " '1073210372541243392',\n",
       " '1079824312708808710',\n",
       " '107m',\n",
       " '108',\n",
       " '1083440238075236353',\n",
       " '1084216851968577536',\n",
       " '1084835531572301829',\n",
       " '10861328',\n",
       " '1086461107244490752',\n",
       " '1086761493503258624',\n",
       " '1087043502670790656',\n",
       " '1088137453268013057',\n",
       " '1088vjf',\n",
       " '108su8n',\n",
       " '1091025377932263427',\n",
       " '1091397656600039425',\n",
       " '1093650615912316928',\n",
       " '1096122681215324168',\n",
       " '109676',\n",
       " '1098295228837048325',\n",
       " '1098748506364203008',\n",
       " '1098954942843211777',\n",
       " '1099247495756996610',\n",
       " '1099419400686321664',\n",
       " '109b',\n",
       " '10a',\n",
       " '10ajmccarron',\n",
       " '10am',\n",
       " '10b',\n",
       " '10b8mm6',\n",
       " '10best',\n",
       " '10downingstreet',\n",
       " '10drvnp',\n",
       " '10dufdm',\n",
       " '10fegys',\n",
       " '10htqii',\n",
       " '10ibnhq',\n",
       " '10inchpolitics',\n",
       " '10k',\n",
       " '10m',\n",
       " '10m1xq4',\n",
       " '10m42s',\n",
       " '10pe',\n",
       " '10pm',\n",
       " '10pme',\n",
       " '10s',\n",
       " '10t',\n",
       " '10th',\n",
       " '10viboi',\n",
       " '10x',\n",
       " '10yo',\n",
       " '11',\n",
       " '110',\n",
       " '1100526297653174274',\n",
       " '1102625638245122048',\n",
       " '1103028675904196613',\n",
       " '1103065871830974465',\n",
       " '1104174354068267013',\n",
       " '1105887640879022080',\n",
       " '1106172145615425538',\n",
       " '1106209135144693760',\n",
       " '1107698956295983105',\n",
       " '1107985087029743617',\n",
       " '1111734163193954305',\n",
       " '1112',\n",
       " '11123018',\n",
       " '11132014',\n",
       " '1113911369978531846',\n",
       " '1114158362571284480',\n",
       " '1114510868027338752',\n",
       " '1115045592911228929',\n",
       " '1116145996743024646',\n",
       " '1116712395844587521',\n",
       " '1119039985431113728',\n",
       " '11192014',\n",
       " '1119228575134015489',\n",
       " '1120434609131532288',\n",
       " '1120702516814123020',\n",
       " '1121110722749112321',\n",
       " '1122844935391252480',\n",
       " '1123939623355518976',\n",
       " '1124841210684813315',\n",
       " '1125006755191246849',\n",
       " '1126593428769378304',\n",
       " '1126625647214964737',\n",
       " '1127561555204349952',\n",
       " '1128637699399868416',\n",
       " '1129713937795076096',\n",
       " '1129822219432071169',\n",
       " '112ci6f',\n",
       " '113',\n",
       " '1130248534169669635',\n",
       " '1130796877803380736',\n",
       " '1130917649326714881',\n",
       " '1131520800014688256',\n",
       " '1131533400798576642',\n",
       " '1131638682027536386',\n",
       " '1132603704689881089',\n",
       " '1132727099930927106',\n",
       " '1132829029197680640',\n",
       " '1133166058515324928',\n",
       " '1133381610915737601',\n",
       " '1134290731793731585',\n",
       " '1134864448869601280',\n",
       " '1138073752162451456',\n",
       " '1138702953194360832',\n",
       " '1138837483653935106',\n",
       " '1138870147907821568',\n",
       " '1139620339355459584',\n",
       " '1139673718039486467',\n",
       " '1139968121475928070',\n",
       " '114',\n",
       " '1140284207673090048',\n",
       " '1140301007840337921',\n",
       " '1140711955252072448',\n",
       " '1141314465188720640',\n",
       " '1143605160989646849',\n",
       " '1143711081531150341',\n",
       " '1144195875654082561',\n",
       " '11442513',\n",
       " '1145',\n",
       " '1145932',\n",
       " '1146187842772766720',\n",
       " '1146552901843578881',\n",
       " '1146917860289208320',\n",
       " '1146918559123218432',\n",
       " '1146923374960107521',\n",
       " '1146946992783994880',\n",
       " '1147165752262311937',\n",
       " '1147215397285978114',\n",
       " '1147221107696709632',\n",
       " '1147423573293830146',\n",
       " '1147481471537430528',\n",
       " '1147694118027612160',\n",
       " '1148636946677358592',\n",
       " '1148962923672494083',\n",
       " '1149350510484475905',\n",
       " '1149478513436573700',\n",
       " '1149505573206937601',\n",
       " '1149669878430744576',\n",
       " '1149738639938732034',\n",
       " '1149883648373477378',\n",
       " '115',\n",
       " '1150056924488052736',\n",
       " '1150153250806685697',\n",
       " '1150453619323363328',\n",
       " '1151468',\n",
       " '1151496',\n",
       " '1152198603240280064',\n",
       " '1152228353040011264',\n",
       " '1154066705284390912',\n",
       " '11550188935',\n",
       " '1155253573959028737',\n",
       " '11553555444',\n",
       " '1155863580136357888',\n",
       " '1155908683349594112',\n",
       " '11564529382',\n",
       " '1156542139460935680',\n",
       " '1156664456849625088',\n",
       " '1157470788758507521',\n",
       " '11586819335',\n",
       " '1159086721591496706',\n",
       " '1159870945630117888',\n",
       " '115gcyg',\n",
       " '116',\n",
       " '1160233516535373825',\n",
       " '1160360266787414017',\n",
       " '1161049221123862528',\n",
       " '1161081413120090113',\n",
       " '1161105208317026309',\n",
       " '1161227143621173248',\n",
       " '1161246021650804737',\n",
       " '1161281498743496704',\n",
       " '1161297400201383937',\n",
       " '1161680797209939968',\n",
       " '1161726809609789442',\n",
       " '1161774305895694336',\n",
       " '1161827348276080641',\n",
       " '1162041355889254400',\n",
       " '1162043613360115713',\n",
       " '1162167406762106881',\n",
       " '1162171796709265408',\n",
       " '1162175486560280577',\n",
       " '1162195296924946433',\n",
       " '1162706070378336257',\n",
       " '1162844956190552065',\n",
       " '1163120378543316992',\n",
       " '1163137145218973696',\n",
       " '1163425829331841024',\n",
       " '1163449741398683649',\n",
       " '1163455592566001665',\n",
       " '1163499292931366913',\n",
       " '1163518397511917568',\n",
       " '1163520717192867840',\n",
       " '1163521046055665664',\n",
       " '1163562360403087373',\n",
       " '1163605746656919552',\n",
       " '1163658628768907265',\n",
       " '1163661278549876736',\n",
       " '1164133869697798144',\n",
       " '1164589842753130496',\n",
       " '1164648591769645056',\n",
       " '1164794',\n",
       " '1164917801947000837',\n",
       " '1164944146534195200',\n",
       " '1165015853324222464',\n",
       " '1165023765476118528',\n",
       " '1165046821275807751',\n",
       " '1165071131608571904',\n",
       " '1165526646448951297',\n",
       " '1165658850286026752',\n",
       " '1165663968591044608',\n",
       " '1165665696250040320',\n",
       " '1165727440280457218',\n",
       " '1166004124627996672',\n",
       " '1166056804276756480',\n",
       " '1166134524952096779',\n",
       " '1166291203022868480',\n",
       " '1166466886558769152',\n",
       " '1166478831747772416',\n",
       " '1166771355653758976',\n",
       " '1166792741445079041',\n",
       " '1167135580372774912',\n",
       " '1167439608638038018',\n",
       " '1167508617857290240',\n",
       " '1167550058411253761',\n",
       " '1167552549974675458',\n",
       " '1167581903542767616',\n",
       " '1167606379097538563',\n",
       " '1169295369609592832',\n",
       " '116qmnr',\n",
       " '1170097018901020673',\n",
       " '1171181065945460736',\n",
       " '1171254828456194049',\n",
       " '1172127718433054721',\n",
       " '1172515960336654336',\n",
       " '1172609171382001665',\n",
       " '1172873180282871808',\n",
       " '1172911375783534594',\n",
       " '1172964660708032512',\n",
       " '1174017425068576769',\n",
       " '1174027975261474817',\n",
       " '1174073517865934849',\n",
       " '11746',\n",
       " '1175364800655740930',\n",
       " '1175766383180648453',\n",
       " '1175804667961511937',\n",
       " '1175807072870420481',\n",
       " '1176',\n",
       " '1176295193877856256',\n",
       " '1176667137831837696',\n",
       " '1176686526006276097',\n",
       " '1176687762768048128',\n",
       " '1176833811625185280',\n",
       " '1176854652823973893',\n",
       " '1176880040761614339',\n",
       " '1177259032412921859',\n",
       " '1177630118577090565',\n",
       " '1177669563477118976',\n",
       " '1177687361808932865',\n",
       " '1177764802363641856',\n",
       " '1177817225937313792',\n",
       " '1178827504301363205',\n",
       " '1179386288748802049',\n",
       " '1179552453492183042',\n",
       " '1179603565129736192',\n",
       " '1179879043874709504',\n",
       " '1179898724899835904',\n",
       " '1179911676537327616',\n",
       " '1179913243151785989',\n",
       " '1179925259417468928',\n",
       " '1179962400554008576',\n",
       " '118',\n",
       " '1180135546607288326',\n",
       " '1180229844078137345',\n",
       " '1180234554356973568',\n",
       " '1180245134363025415',\n",
       " '1180264901161472000',\n",
       " '1180307049827176453',\n",
       " '1180524380985679872',\n",
       " '1180539962611163136',\n",
       " '1180864952128671744',\n",
       " '1180yb8',\n",
       " '1181053382850514947',\n",
       " '1181319565235634177',\n",
       " '1181362650690867202',\n",
       " '1181369975677816832',\n",
       " '1181389560661577730',\n",
       " '1181559221453819904',\n",
       " '1181612030027030529',\n",
       " '1181682375425712128',\n",
       " '1181696544929697794',\n",
       " '1181704952273657857',\n",
       " '1181890573151678464',\n",
       " '1181904774209384450',\n",
       " '1181913815073603585',\n",
       " '1181914827117215744',\n",
       " '1181918781259571200',\n",
       " '1181926019692974081',\n",
       " '1181969511697788928',\n",
       " '1182043375815417856',\n",
       " '1182259309540990976',\n",
       " '1182303821407670274',\n",
       " '1182358854094544901',\n",
       " '1182470552818241536',\n",
       " '1182485222190915584',\n",
       " '1182663741827178497',\n",
       " '1182736397926588423',\n",
       " '1182758375509123072',\n",
       " '1183113326374600705',\n",
       " '1183115133456998400',\n",
       " '1183399176119443457',\n",
       " '1184198558007869440',\n",
       " '1184270536437915648',\n",
       " '1184283019768713221',\n",
       " '1184288405481541632',\n",
       " '1184297277541797888',\n",
       " '1184466327974617088',\n",
       " '1184495506195697665',\n",
       " '1184497591679107072',\n",
       " '1184835631408697344',\n",
       " '1184880769099096064',\n",
       " '1184924506747482112',\n",
       " '1185164957408792582',\n",
       " '1185370501809627136',\n",
       " '1185374577662271488',\n",
       " '1185385572937981952',\n",
       " '1185555345197977602',\n",
       " '1185656366138089478',\n",
       " '1186035774984245249',\n",
       " '1186365994035482624',\n",
       " '1186371416461066242',\n",
       " '1186617655479943168',\n",
       " '1186748559288340480',\n",
       " '1186817538782572544',\n",
       " '1187111493311107073',\n",
       " '1187176853599150081',\n",
       " '1187763371837415424',\n",
       " '1187795599241949184',\n",
       " '1187820031645798401',\n",
       " '1188636122764718080',\n",
       " '1188848384003604481',\n",
       " '1188856716638773255',\n",
       " '1188914973638057989',\n",
       " '1189228108022460417',\n",
       " '1189589231963987968',\n",
       " '1189600719957049345',\n",
       " '1189601417469841409',\n",
       " '1189706738838228998',\n",
       " '1189948192986714118',\n",
       " '118m',\n",
       " '119',\n",
       " '1190057966520668162',\n",
       " '1190304473190731778',\n",
       " '1190606019149340674',\n",
       " '1190616779049127941',\n",
       " '1190638811551862784',\n",
       " '1190715683681820672',\n",
       " '1190790447452577798',\n",
       " '1190841385345310722',\n",
       " '1191193570763563013',\n",
       " '1191194361452810241',\n",
       " '1191563608850911232',\n",
       " '1191752174084210699',\n",
       " '1191906297630859265',\n",
       " '1191937581711265794',\n",
       " '1192461425358385154',\n",
       " '1192640102943674368',\n",
       " '1192754718973530114',\n",
       " '1192834231707283457',\n",
       " '1193030111383228416',\n",
       " '1193206698745438209',\n",
       " '1193280422173708288',\n",
       " '1193327464308625409',\n",
       " '1193546581288636417',\n",
       " '1193581836691226625',\n",
       " '1193843654647058433',\n",
       " '1193971530297946113',\n",
       " '1194269692820635648',\n",
       " '1194448682466398209',\n",
       " '1194696248223240193',\n",
       " '1194733225249083392',\n",
       " '1195379578358960128',\n",
       " '1195408797633671168',\n",
       " '1195423988714999808',\n",
       " '1195429727370174469',\n",
       " '1195459246382616576',\n",
       " '1195493031715184645',\n",
       " '1195676437954670592',\n",
       " '1195687574074904576',\n",
       " '1195845039244623878',\n",
       " '1196158817408602113',\n",
       " '119662',\n",
       " '1196819014527410177',\n",
       " '1196923812136267777',\n",
       " '1196931156551311361',\n",
       " '1196948204241207296',\n",
       " '1197293250115014656',\n",
       " '1197305095970418688',\n",
       " '1197635945350254595',\n",
       " '11978141',\n",
       " '1197884074540650498',\n",
       " '1197943099781201922',\n",
       " '1198240749688676352',\n",
       " '1199187277450227712',\n",
       " '11a',\n",
       " '11am',\n",
       " '11bldev',\n",
       " '11catlm',\n",
       " '11cbssr',\n",
       " '11dinvl',\n",
       " '11e2',\n",
       " '11e4',\n",
       " '11e5',\n",
       " '11e6',\n",
       " '11e7',\n",
       " '11fhpr',\n",
       " '11glrmi',\n",
       " '11gmnko',\n",
       " '11m',\n",
       " '11n3atd',\n",
       " '11n5pgy',\n",
       " '11ngh6l',\n",
       " '11nt9z2',\n",
       " '11p4mhy',\n",
       " '11phenomenon',\n",
       " '11pm',\n",
       " '11rqiac',\n",
       " '11s',\n",
       " '11sheila11',\n",
       " '11summerb',\n",
       " '11th',\n",
       " '11thhttp',\n",
       " '11u1lzb',\n",
       " '11ubrck',\n",
       " '11x8jpb',\n",
       " '11yeyrd',\n",
       " '11zjf8n',\n",
       " '11zq8c3',\n",
       " '11zsnt7',\n",
       " '12',\n",
       " '120',\n",
       " '1200189112952795139',\n",
       " '1200593444944654336',\n",
       " '1200871391517650944',\n",
       " '1200892525931909120',\n",
       " '1200919325152034816',\n",
       " '1201216057924558848',\n",
       " '1201499577645449216',\n",
       " '1201525451367354370',\n",
       " '1201553654358200320',\n",
       " '1201590807',\n",
       " '1201591884',\n",
       " '1201912832272601088',\n",
       " '1201975346242957314',\n",
       " '1202772467510579201',\n",
       " '1203267964818067457',\n",
       " '1203337505552109568',\n",
       " '1203419642385043457',\n",
       " '1203519682038435840',\n",
       " '1203545049059020800',\n",
       " '1203666657769598976',\n",
       " '1203696367178309632',\n",
       " '1203855761690038273',\n",
       " '1204036130791985153',\n",
       " '1204083990501740546',\n",
       " '1204101738325823490',\n",
       " '1204417416446373889',\n",
       " '1204520225607225344',\n",
       " '1204530385817939968',\n",
       " '1204569081158000641',\n",
       " '1204578501409554437',\n",
       " '1204787517402353664',\n",
       " '1204797730641629188',\n",
       " '1204810172813729796',\n",
       " '1204839185737244672',\n",
       " '1204881311514669059',\n",
       " '1204894883410010119',\n",
       " '1204960786013024257',\n",
       " '1204988423230111744',\n",
       " '1205581534071271424',\n",
       " '1205596836708278272',\n",
       " '1205930681970417664',\n",
       " '1205989416927453186',\n",
       " '1205996212542955520',\n",
       " '1206071149005869056',\n",
       " '1206077319187587073',\n",
       " '1206323905704710144',\n",
       " '1206372649510850562',\n",
       " '1207076232849960960',\n",
       " '120708',\n",
       " '1207299582205542402',\n",
       " '1207359122372476928',\n",
       " '1207411432565956609',\n",
       " '1207479011464626176',\n",
       " '1207487262163951616',\n",
       " '1207492122548621312',\n",
       " '1207494537544896514',\n",
       " '1207609959241932800',\n",
       " '1207640058276265984',\n",
       " '1208077788323831809',\n",
       " '1208130142721040384',\n",
       " '1208133195205660675',\n",
       " '1208186990400692224',\n",
       " '1208196888559788033',\n",
       " '1208771819605508096',\n",
       " '1208776640362356737',\n",
       " '1209199407025053697',\n",
       " '120k',\n",
       " '120m',\n",
       " '120th',\n",
       " '121',\n",
       " '1210059718452031488',\n",
       " '1210183552375107584',\n",
       " '1210247652178313217',\n",
       " '1210570523970850816',\n",
       " '1210572716698488839',\n",
       " '1210602372680077315',\n",
       " '1210632030943211520',\n",
       " '1210679166170890240',\n",
       " '1210679968230821889',\n",
       " '1210683656097947648',\n",
       " '1210694529197756416',\n",
       " '1210701262498361344',\n",
       " '1210750161174941696',\n",
       " '1210781228149817355',\n",
       " '1211',\n",
       " '1211767146637930500',\n",
       " '1211834807862185985',\n",
       " '1212042598732439554',\n",
       " '1212055588550524928',\n",
       " '1212117724626862080',\n",
       " '1212120082832412678',\n",
       " '1212139414312275968',\n",
       " '1212171925520363521',\n",
       " '1212376966873206786',\n",
       " '1212391791774838784',\n",
       " '1212402251806990336',\n",
       " '1212475089133944832',\n",
       " '1213583859402657793',\n",
       " '1214207553971924996',\n",
       " '1214230745176322048',\n",
       " '1214608814185631749',\n",
       " '1214618992930959360',\n",
       " '1215112427756498945',\n",
       " '1215238525840429056',\n",
       " '1215259431166005248',\n",
       " '1215387768039317506',\n",
       " '1216212972391096320',\n",
       " '1216236014835920896',\n",
       " '1216356280933273600',\n",
       " '1216890283666890753',\n",
       " '1216896462858022912',\n",
       " '1217295090739228674',\n",
       " '1217508926872334336',\n",
       " '1217793784198639618',\n",
       " '1217855192399601665',\n",
       " '1217888605961506816',\n",
       " '1217927213179338758',\n",
       " '1218141834842660864',\n",
       " '1218181208909193220',\n",
       " '1218313039432290305',\n",
       " '1219329693863772160',\n",
       " '1219331040361271297',\n",
       " '1219708789957578752',\n",
       " '1219736663930417155',\n",
       " '1219790888714612738',\n",
       " '1219826046125641733',\n",
       " '1219909745450065920',\n",
       " '122',\n",
       " '1220047531272364037',\n",
       " '1220103171403665410',\n",
       " '1220364541046992899',\n",
       " '1220427121685864448',\n",
       " '1220466780524765187',\n",
       " '1220531644773199872',\n",
       " '1220712020263276545',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_from_pipe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients = pipe.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnhpV6P7YrNx"
   },
   "source": [
    "Using these extracted components above, display the 5 words with the highest coefficients and the 5 words with the smallest coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPok-Fh0YrNx"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_RfCSDeYrNx"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gDTzDxPYrNy"
   },
   "source": [
    "#### 2(f)\n",
    "rubric={points:10}\n",
    "\n",
    "scikit-learn provides a lot of useful tools like `make_pipeline` and `cross_validate`, which are awesome. But with these fancy tools it's also easy to lose track of what is actually happening under the hood. Here, your task is to \"manually\" (without `Pipeline` and without `cross_validate` or `cross_val_score`) compute logistic regression's validation score on one fold (that is, train on 80% and validate on 20%) of the training data. \n",
    "\n",
    "You should start with the following `CountVectorizer` and `LogisticRegression` objects, as well as `X_train` and `y_train` (which you should further split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5uV9CUn6YrNy"
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]\n",
    "y = tweets_df[\"retweets\"] > 10_000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = countvec.fit_transform(X_train)\n",
    "X_test = countvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, X_test, y_test, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8777546210889848"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8978203206089264"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyELnkIEYrNy"
   },
   "source": [
    "Meta-comment: you might be wondering why we're going into \"implementation\" here if this course is about _applied_ ML. In CPSC 340, we would go all the way down into `LogisticRegression` and understand how `fit` works, line by line. Here we're not going into that at all, but I still think this type of question (and Exercise 1) is a useful middle ground. I do want you to know what is going on in `Pipeline` and in `cross_validate` even if we don't cover the details of `fit`. To get into logistic regression's `fit` requires a bunch of math; here, we're keeping it more conceptual and avoiding all those prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RlhhIWSYrNy"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmm9RXWpYrNy"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjOoiWcaYrNz",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 3: hyperparameter optimization\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnZtJxCFYrNz",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e9e6fdea209d872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3(a)\n",
    "rubric={points:4}\n",
    "\n",
    "The following code varies the `max_features` hyperparameter of `CountVectorizer` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `max_features`. It also prints the results. Based on the plot/output, what value of `max_features` seems best? Briefly explain.\n",
    "\n",
    "Note: the code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<34681x42807 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 395886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ASVVU2GJYrNz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100\n",
      "1000\n",
      "10000\n",
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "max_features = [10, 100, 1000, 10_000, 100_000]\n",
    "\n",
    "for mf in max_features:\n",
    "    print(mf)\n",
    "    pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\", max_features=mf),\n",
    "        LogisticRegression(max_iter=1000),\n",
    "    )\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "    \n",
    "plt.semilogx(max_features, train_scores, label=\"train\");\n",
    "plt.semilogx(max_features, cv_scores, label=\"valid\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"max_features\");\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7Xj9GOqAYrNz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>train</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features  train  cv\n",
       "0            10    NaN NaN\n",
       "1           100    NaN NaN\n",
       "2          1000    NaN NaN\n",
       "3         10000    NaN NaN\n",
       "4        100000    NaN NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"max_features\": max_features, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i8B9DSWYrN0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdpAQlc9YrN0"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYWMfXjDYrN0"
   },
   "source": [
    "#### 3(b)\n",
    "rubric={points:4}\n",
    "\n",
    "The following code varies the `C` hyperparameter of `LogisticRegression` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `C`. Based on the plot, what value of `C` seems best?\n",
    "\n",
    "Note: the code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Kf2Oo0HZYrN0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 771, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "C_vals = 10.0 ** np.arange(-1.5, 2, 0.5)\n",
    "\n",
    "for C in C_vals:\n",
    "    #     print(C)\n",
    "    pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\", max_features=None),\n",
    "        LogisticRegression(max_iter=1000, C=C),\n",
    "    )\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(C_vals, train_scores, label=\"train\")\n",
    "plt.semilogx(C_vals, cv_scores, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "q3mxFnohYrN0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>train</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.622777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           C  train  cv\n",
       "0   0.031623    NaN NaN\n",
       "1   0.100000    NaN NaN\n",
       "2   0.316228    NaN NaN\n",
       "3   1.000000    NaN NaN\n",
       "4   3.162278    NaN NaN\n",
       "5  10.000000    NaN NaN\n",
       "6  31.622777    NaN NaN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"C\": C_vals, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQisBcRIYrN1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyeNRlF0YrN1"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMJV_v5FYrN1"
   },
   "source": [
    "#### 3(c)\n",
    "rubric={points:12}\n",
    "\n",
    "- Using `GridSearchCV`, jointly optimize `max_features` and `C` across all the combinations of values we tried above. \n",
    "  - Note: the code might be a bit slow here. \n",
    "  - Setting `n_jobs=-1` should speed it up if you have a multi-core processor.\n",
    "  - You can reduce the number of folds (e.g. `cv=2`) to speed it up if necessary.\n",
    "- What are the best values of `max_features` and `C` according to your grid search?\n",
    "- Do these best values agree with what you found in parts (a) and (b)?\n",
    "- Generally speaking, _should_ these values agree with what you found in parts (a) and (b)? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjKQPENCYrN1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\"),\n",
    "        LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'logisticregression__C': 10.0 ** np.arange(-1.5, 2, 0.5), \n",
    "           'countvectorizer__max_features': [10, 100, 1000, 10_000, 100_000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('countvectorizer',\n",
       "                                        CountVectorizer(stop_words='english')),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'countvectorizer__max_features': [10, 100, 1000,\n",
       "                                                            10000, 100000],\n",
       "                          'logisticregression__C': array([ 0.03162278,  0.1       ,  0.31622777,  1.        ,  3.16227766,\n",
       "       10.        , 31.6227766 ])}])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator = pipe, param_grid = params , n_jobs = -1, cv = 3)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countvectorizer__max_features': 100000, 'logisticregression__C': 1.0}\n",
      "score = 0.8971771592775974\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(\"score =\",grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA7LqKMiYrN1"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dowrECydYrN2"
   },
   "source": [
    "#### 3(d)\n",
    "rubric={points:5}\n",
    "\n",
    "- Evaluate your final model on the test set. \n",
    "- How does your test accuracy compare to your validation accuracy? \n",
    "- If they are different: do you think this is because you \"overfitted on the validation set\", or simply random luck?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWR3ox3DYrN2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]\n",
    "y = tweets_df[\"retweets\"] > 10_000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\", max_features = 100000),\n",
    "        LogisticRegression(max_iter=1000, C = 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvectorizer',\n",
       "                 CountVectorizer(max_features=100000, stop_words='english')),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986276092722869"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mDaFYsPYrN2"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTP8_-49YrN2"
   },
   "source": [
    "## Exercise 4: Very short answer questions\n",
    "rubric={points:10}\n",
    "\n",
    "Each question is worth 2 points. Max 2 sentences per answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YSHbjMkYrN2"
   },
   "source": [
    "1. What is the problem with calling `fit_transform` on your test data with `CountVectorizer`? \n",
    "2. Why is it important to follow the Golden Rule? If you violate it, will that give you a worse classifier?\n",
    "3. If you could only access one of `predict` or `predict_proba`, which one would you choose? Briefly explain.\n",
    "4. What are two advantages of using sklearn `Pipeline`s? \n",
    "5. What are two advantages of `RandomizedSearchCV` over `GridSearchCV`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcn0VUmZYrN3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPzVWykRYrN3"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwNvQfgpYrN3"
   },
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
